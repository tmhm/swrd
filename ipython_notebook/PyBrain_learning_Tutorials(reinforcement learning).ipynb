{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization in reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.rl.environments.cartpole.balancetask import BalanceTask\n",
    "task = BalanceTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "net = buildNetwork(task.outdim, 3, task.indim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.rl.agents import OptimizationAgent\n",
    "from pybrain.rl.experiments import EpisodicExperiment\n",
    "from pybrain.optimization import HillClimber\n",
    "agent = OptimizationAgent(net, HillClimber())\n",
    "exp = EpisodicExperiment(task, agent)\n",
    "exp.doEpisodes(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, -1, -1, -1, -1, -1, -1, -1984],\n",
       " [-1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1976],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1982],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1980],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1968],\n",
       " [-1, -1, -1, -1, -1, -1988],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1978],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1982],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1978],\n",
       " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1976]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pybrain.rl.learners import ENAC\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "agent = LearningAgent(net, ENAC())\n",
    "exp = EpisodicExperiment(task, agent)\n",
    "exp.doEpisodes(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "官网的example, from Pybrain v0.3 [documentation](http://pybrain.org/docs/tutorial/reinforcement-learning.html)\n",
    "\n",
    "**Note: You can directly run the code in this tutorial by running the script docs/tutorials/rl.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "\n",
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%clear\n",
    "%reset\n",
    "from scipy import *\n",
    "import sys, time \n",
    "\n",
    "from pybrain.rl.environments.mazes import Maze,MDPMazeTask\n",
    "from pybrain.rl.learners.valuebased import ActionValueTable\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "from pybrain.rl.learners import Q, SARSA\n",
    "from pybrain.rl.experiments import Experiment\n",
    "from pybrain.rl.environments import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "A reinforcement learning (RL) task in pybrain always consists of a few\n",
    "components that interact with each other: Environment, Agent, Task, and\n",
    "Experiment. In this tutorial we will go through each of them, create\n",
    "the instances and explain what they do.\n",
    "\n",
    "But first of all, we need to import some general packages and the RL\n",
    "components from PyBrain:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pylab \n",
    "pylab.gray()\n",
    "pylab.ion()\n",
    "%matplotlib inline\n",
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later visualization purposes, we also need to initialize the\n",
    "plotting engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment is the world, in which the agent acts. It receives input\n",
    "with the** .performAction()** method and returns an output with\n",
    "**.getSensors().** All environments in PyBrain are located under\n",
    "*pybrain/rl/environments.*\n",
    "\n",
    "One of these environments is the maze environment, which we will use for\n",
    "this tutorial. It creates a labyrinth with free fields, walls, and an\n",
    "goal point. An agent can move over the free fields and needs to find the\n",
    "goal point. Let's define the maze structure, a simple 2D numpy array, where\n",
    "1 is a wall and 0 is a free field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structure = array( [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                    [1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "                    [1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "                    [1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "                    [1, 0, 0, 1, 0, 1, 1, 0, 1],\n",
    "                    [1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "                    [1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "                    [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "                    [1, 1, 1, 1, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  goal:\n",
    "  \n",
    "  (n,0),(n,1),(n,2),...(n,n)\n",
    "  \n",
    "  ...\n",
    "  \n",
    "  ...\n",
    "  \n",
    "  (0,0),(0,1),(0,2),...(0,n)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "environmet = Maze(structure, (1,1)) #(1.1) goal position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########\n",
      "#@      #\n",
      "####### #\n",
      "#     # #\n",
      "#  # ## #\n",
      "#  #  # #\n",
      "#  #  # #\n",
      "#* #    #\n",
      "#########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print environmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "environmet??"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "controller = ActionValueTable(81,4)\n",
    "controller.initialize(1.)\n",
    "#print controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# controller.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = Q()\n",
    "agent = LearningAgent(controller, learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task = MDPMazeTask(environmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(task, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD8BJREFUeJzt3V+s5GV9x/H3Z2Znd1lgUSuxKoKShogmBKlVK9BoQQWs\nGkkaRRNT0tgL20JsYtp4Q3rRCy+sNWmThqgUKVjjKtUm0IDBaLRxQVkEZKmtlLKAEIn8qSGHHOZ8\nezGzyzl7zjK/YXfO7wHer2Ry5nf2mTmfPX8+55lnfnOeVBWSpHYN+g4gSXp2FrUkNc6ilqTGWdSS\n1DiLWpIaZ1FLUuM6FXWSS5PcMb1csuhQkqRnzCzqJG8E/hh4M3A68AdJTl50MEnSRJcZ9anA7qp6\nqqrGwPeACxcbS5K0X5eivhM4O8lLk+wALgBes9hYkqT9tswaUFV3J/kMcCPwa2APMF50MEnSROb9\nWx9J/gbYV1X/eND7/aMhkjSnqsqsMTNn1ABJjq+qXyY5Efgg8LaNxm3fvn2+hAu2vLzMaDTqO8Ya\nZuqmxUzQZi4zddNipqWlpU7jOhU18PUkLwOWgU9U1RPPNZgkaT6dirqqfm/RQSRJG3tBvzJxMGjv\nv2emblrMBG3mMlM3LWbqau4nEw95R0m1tkYtSS1bWlrq9GTi8/dXjCS9SFjUktQ4i1qSGmdRS1Lj\nLGpJapxFLUmNs6glqXEWtSQ1zqKWpMZZ1JLUOItakhpnUUtS4yxqSWqcRS1JjetU1Ek+meTOJLcn\nuTrJ1kUHkyRNzCzqJK8C/hw4o6pOY7IrzIcXHUySNNF1z8QhcHSSFWAH8ODiIkmSVps5o66qB4HP\nAvcBDwCPVdW3Fx1MkjTRZenjJcAHgJOAVwHHJPnIRmOXl5cPXMbj8ZFNKknPc+PxeE1PdtVl6eNc\n4J6q+hVAkm8AbweuOXjgaDTq/IEXbWlpqe8IkhrSwp6uw+GQ4XB44LjrhLbLWR/3AW9Lsj1JgHOA\nvc8lpCRpfl3WqG8GdgF7gJ8AAS5fcC5J0lSq6sjcUVItPLTYz6UPSau11E/7LS0tUVWZNc5XJkpS\n4yxqSWqcRS1JjbOoJalxFrUkNc6ilqTGWdSS1DiLWpIaZ1FLUuMsaklqnEUtSY2zqCWpcRa1JDXO\nopakxlnUktQ4i1qSGtdlc9tTkuxJcuv07eNJLtmMcJKkDpvbVtXPgDcBJBkA9wPXLjiXJGlq3qWP\nc4GfV9W+RYSRJK03b1F/CPjKIoJIkjbWeXPbJCPgQeANVfXLDf69hsPhgePBYMDq483m5raSVmth\nc9vxeMzKysqa4y6b285co17lfODHG5X0fqPRaI67W6wWvijSi5GTpEMbDodrJrDj8bjT7eZZ+rgI\nlz0kadN1KuokO5g8kfiNxcaRJB2s09JHVT0JHL/gLJKkDfjKRElqnEUtSY2zqCWpcRa1JDXOopak\nxlnUktQ4i1qSGmdRS1LjLGpJapxFLUmNs6glqXEWtSQ1zqKWpMZZ1JLUOItakhpnUUtS47ru8HJc\nkq8l2Zvkp0neuuhgkqSJrpvbfh64rqr+MMkWYMcCM0mSVplZ1El2AmdX1R8BVNXTwBMLziVJmuqy\n9PE64JEkVyS5NcnlSY5adDBJ0kSq6tkHJL8N/BD43ar6UZK/Ax6vqssOGlfD4fDA8WAwYPXxZjvr\nrLN6+9iH8r73va/vCBvat29f3xHWefTRR/uOsM5VV13Vd4R1BoP2zgdYWlrqO8KGtm/f3ncExuMx\nKysra46rKrNu1+WrfD+wr6p+ND3eBZyx0cDRaHTg0mdJS1KLhsPhmp7samZRV9XDwL4kp0zfdQ5w\n13OLKUmaV9ezPi4Brk4yAu4BLl5cJEnSap2Kuqp+AvzOgrNIkjbQ3jMRkqQ1LGpJapxFLUmNs6gl\nqXEWtSQ1zqKWpMZZ1JLUOItakhpnUUtS4yxqSWqcRS1JjbOoJalxFrUkNc6ilqTGWdSS1LhOf486\nyb3A48AKsFxVb1lkKEnSM7ru8LICvKOq2ttxVJJe4LoufWSOsZKkI6hr+RZwY5Jbknx8kYEkSWt1\nXfo4s6p+keR4JoW9t6q+v8hgkqSJVNV8N0guA/6vqv72oPfXcDg8cDwYDFh9vNl2797d28c+lAce\neKDvCBu68MIL+46wzvnnn993hHWuv/76viM8Lzz11FN9R9jQtm3b+o7AeDxmZWVlzXFVZdbtZi59\nJNmR5Jjp9aOBdwN3bjR2NBoduPRZ0pLUouFwuKYnu+qy9PEK4NokNR1/dVXd8BxzSpLmNLOoq+p/\ngNM3IYskaQOecidJjbOoJalxFrUkNc6ilqTGWdSS1DiLWpIaZ1FLUuMsaklqnEUtSY2zqCWpcRa1\nJDXOopakxlnUktQ4i1qSGmdRS1LjLGpJalznok4ySHJrkm8tMpAkaa15ZtSXAnctKogkaWOdijrJ\nCcAFwBcWG0eSdLCuM+rPAZ8CaoFZJEkbmFnUSd4LPFxVtwGZXiRJm2TmLuTAmcD7k1wAHAUcm+TL\nVfWxgwcuLy8fuD4YDBgOh0cs6LxOO+203j72oRx77LF9R3jeeOyxx/qOsM7OnTv7jrDOE0880XcE\nzWE8HrOysjL37WbOqKvq01V1YlWdDHwYuGmjkgYYjUYHLn2WtCS1aDgcrunJrjyPWpIa12Xp44Cq\n+i7w3QVlkSRtwBm1JDXOopakxlnUktQ4i1qSGmdRS1LjLGpJapxFLUmNs6glqXEWtSQ1zqKWpMZZ\n1JLUOItakhpnUUtS4yxqSWqcRS1JjbOoJalxMzcOSLIN+B6wdTp+V1X99aKDSZImZhZ1VT2V5J1V\n9WSSIfCDJNdX1c2bkE+SXvQ6LX1U1ZPTq9uYlHstLJEkaY1ORZ1kkGQP8BBwY1XdsthYkqT9Om1u\nW1UrwJuS7AT+Nckbququg8ctLy8fuD4YDBgOh0csqCQ9343HY1ZWVua+3by7kD+R5DvAecC6oh6N\nRnMHWJR777237wjrDAaeZNPVI4880neEdfz6dZOk7wjNGg6Hayaw4/G40+1mfucleXmS46bXjwLe\nBdz93GJKkubVZUb9SuDKJAMmxf7VqrpusbEkSft1OT3vDuCMTcgiSdqAi26S1DiLWpIaZ1FLUuMs\naklqnEUtSY2zqCWpcRa1JDXOopakxlnUktQ4i1qSGmdRS1LjLGpJapxFLUmNs6glqXEWtSQ1zqKW\npMZ12YrrhCQ3JflpkjuSXLIZwSRJE1224noa+Iuqui3JMcCPk9xQVe6bKEmbYOaMuqoeqqrbptd/\nDewFXr3oYJKkibnWqJO8Fjgd2L2IMJKk9bosfQAwXfbYBVw6nVmvs7y8fOD6YDBgOBwedkBJeqEY\nj8esrKzMfbtORZ1kC5OSvqqqvnmocaPRaO4Ai3Lqqaf2HWGdK6+8su8Izxt79+7tO8I6W7du7TvC\nOk8//XTfETSH4XC4ZgI7Ho873a7r0seXgLuq6vPzR5MkHY4up+edCXwU+P0ke5LcmuS8xUeTJEGH\npY+q+gHgYrMk9cRXJkpS4yxqSWqcRS1JjbOoJalxFrUkNc6ilqTGWdSS1DiLWpIaZ1FLUuMsaklq\nnEUtSY2zqCWpcRa1JDXOopakxlnUktQ4i1qSGtdlh5cvJnk4ye2bEUiStFaXGfUVwHsWHUSStLGZ\nRV1V3wce3YQskqQNuEYtSY1LVc0elJwE/FtVnfYsY2o4fGYP3MFgwOpjSXqxG4/HrKysrDmuqsy6\n3cxdyOcxGo2O5N1J0gvKcDhcM4Edj8edbtd16SPTiyRpk3U5Pe8a4D+AU5Lcl+TixceSJO3XaY26\n0x0ltX379iNyX5L0YrC0tNRpjdqzPiSpcRa1JDXOopakxlnUktQ4i1qSGmdRS1LjLGpJapxFLUmN\ns6glqXEWtSQ1zqKWpMZZ1JLUOItakhpnUUtS4yxqSWpcp6JOcl6Su5P8LMlfLjqUJOkZXXZ4GQB/\nD7wHeCNwUZLXLzrYkdB1P7LNZKZuWswEbeYyUzctZuqqy4z6LcB/VdX/VtUy8C/ABxYb68hYvdtv\nK8zUTYuZoM1cZuqmxUxddSnqVwP7Vh3fP32fJGkT+GSiJDVuS4cxDwAnrjo+Yfq+dZaWlo5EpiOq\nxXUpM3XTYiZoM5eZumkxUxczdyFPMgT+EzgH+AVwM3BRVe1dfDxJ0swZdVWNk/wZcAOTpZIvWtKS\ntHlmzqglSf067CcTW3wxTJIvJnk4ye19Z9kvyQlJbkry0yR3JLmkgUzbkuxOsmea6bK+M+2XZJDk\n1iTf6jsLQJJ7k/xk+rm6ue88AEmOS/K1JHun31dvbSDTKdPP0a3Tt4838r3+ySR3Jrk9ydVJtjaQ\n6dLpz93sPqiq53xhUvT/DZwEjIDbgNcfzn0eiQtwFnA6cHvfWVZl+k3g9On1Y5is+7fwudoxfTsE\nfgi8pe9M0zyfBP4Z+FbfWaZ57gFe2neOgzL9E3Dx9PoWYGffmQ7KNwAeBF7Tc45XTb9+W6fHXwU+\n1nOmNwK3A9umP3s3ACcfavzhzqibfDFMVX0feLTvHKtV1UNVddv0+q+BvTRwPnpVPTm9uo3JD3vv\na2FJTgAuAL7Qd5ZVQkOnsybZCZxdVVcAVNXTVfVEz7EOdi7w86raN3Pk4g2Bo5NsAXYw+QXSp1OB\n3VX1VFWNge8BFx5q8OF+4/limOcgyWuZzPh395vkwBLDHuAh4MaquqXvTMDngE/RwC+NVQq4Mckt\nST7edxjgdcAjSa6YLjNcnuSovkMd5EPAV/oOUVUPAp8F7mNyavFjVfXtflNxJ3B2kpcm2cFkYvKa\nQw1uZobwYpHkGGAXcOl0Zt2rqlqpqjcxOT/+rUne0GeeJO8FHp4++sj00oIzq+oMJj9Qf5rkrJ7z\nbAHOAP5hmutJ4K/6jfSMJCPg/cDXGsjyEiaP9E9isgxyTJKP9Jmpqu4GPgPcCFwH7AEOeZL34RZ1\n5xfDCKYPu3YBV1XVN/vOs9r0YfN3gPN6jnIm8P4k9zCZjb0zyZd7zkRV/WL69pfAtUyW/fp0P7Cv\nqn40Pd7FpLhbcT7w4+nnq2/nAvdU1a+mywzfAN7ecyaq6oqqenNVvQN4DPjZocYeblHfAvxWkpOm\nz6J+GGjiWXramo3t9yXgrqr6fN9BAJK8PMlx0+tHAe8C7u4zU1V9uqpOrKqTmXw/3VRVH+szU5Id\n00dCJDkaeDeTh669qaqHgX1JTpm+6xzgrh4jHewiGlj2mLoPeFuS7UnC5HPV+2tBkhw/fXsi8EHg\nmkON7fIS8kOqRl8Mk+Qa4B3AbyS5D7hs/5MuPWY6E/gocMd0TbiAT1fVv/cY65XAldM/ZTsAvlpV\n1/WYp1WvAK5NUkx+Zq6uqht6zgRwCXD1dJnhHuDinvMAk19sTGaxf9J3FoCqujnJLibLC8vTt5f3\nmwqAryd5GZNMn3i2J4N9wYskNc4nEyWpcRa1JDXOopakxlnUktQ4i1qSGmdRS1LjLGpJapxFLUmN\n+39FP8/Lx0IAdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc76d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    experiment.doInteractions(100)\n",
    "    agent.learn()\n",
    "    agent.reset()\n",
    "    \n",
    "pylab.pcolor(controller.params.reshape(81, 4).max(1).reshape(9,9))\n",
    "pylab.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pylab.pcolor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[ 3  6  9 12]\n"
     ]
    }
   ],
   "source": [
    "a= array([[1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9],\n",
    "        [10,11,12]])\n",
    "print a\n",
    "b= a.max(1)  # 0,按列取最大；  1， 按行取最大\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-9368aec6d5fc>, line 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-9368aec6d5fc>\"\u001b[1;36m, line \u001b[1;32m69\u001b[0m\n\u001b[1;33m    \"\"\"Construct a binary tree of depth n for forseeing a depth of n\"\"\"\u001b[0m\n\u001b[1;37m                                                                       \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "class QBinaryTree(object):\n",
    "    def __init__(self):\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.parent = None\n",
    "        self.reward = 0\n",
    "        self.value = None\n",
    "        self.depth = None\n",
    "        self.Qval = 0\n",
    "        self.Gdata = []\n",
    "    def setLeft(self, node):\n",
    "        self.left = node\n",
    "        self.left.value = \"Discharge\"\n",
    "    def setRight(self, node):\n",
    "        self.right = node\n",
    "        self.right.value = \"Charge\"\n",
    "    def setParent(self, node):\n",
    "        self.parent = node\n",
    "    def getParent(self):\n",
    "        return self.parent\n",
    "    def getLeft(self):\n",
    "        return self.left\n",
    "    def getRight(self):\n",
    "        return self.right\n",
    "    def getTReward(self, Pwt, Dt, Rtc, Rtd, k, gamma):\n",
    "        if self.getDepth() == 3:\n",
    "\n",
    "            temp = self\n",
    "            reward = 0\n",
    "                while temp.getParent():\n",
    "                    n = temp.getDepth()\n",
    "                    n -= 1\n",
    "                    if temp.value == \"Charge\":\n",
    "                        reward += gamma ** (n) * k * (Pwt[n] - Rtc)\n",
    "\n",
    "                    if temp.value == \"Discharge\":\n",
    "                        reward += gamma ** (n) * (Pwt[n] / Dt[n]) * (Dt[n] - Rtd)\n",
    "\n",
    "                    temp = temp.getParent()\n",
    "                return reward\n",
    "        else:\n",
    "            print(\"Not to be used for this purpose\")\n",
    " \n",
    "  def getDepth(self):\n",
    "        cnt = 0\n",
    "        temp = self\n",
    "        while temp.getParent():\n",
    "            cnt += 1\n",
    "            temp = temp.getParent()\n",
    "\n",
    "        return cnt\n",
    " \n",
    "    def __str__(self):\n",
    "        stringer = []\n",
    "        tempstring = \"\"\n",
    "        temp = self\n",
    "        while temp.getParent():\n",
    "            stringer.append(temp.value)\n",
    "            temp = temp.getParent()\n",
    "        stringer.reverse()\n",
    "        for i in range(len(stringer)):\n",
    "            tempstring += stringer[i] + \">>\"\n",
    "        return tempstring\n",
    " \n",
    "    def DepnBtree(number):\n",
    "        \"\"\"Construct a binary tree of depth n for forseeing a depth of n\"\"\"\n",
    "        number = 2 ** (number + 1) - 1\n",
    "        root = QBinaryTree()\n",
    "        queue = [root]\n",
    "        counter = 1\n",
    "        status = True\n",
    "        while status:\n",
    "            if len(queue) > 0 and counter != number:\n",
    "                temp = queue.pop(0)\n",
    "                temp.setLeft(QBinaryTree())\n",
    "                temp.setRight(QBinaryTree())\n",
    "                temp.getLeft().setParent(temp)\n",
    "                temp.getRight().setParent(temp)\n",
    "                queue.append(temp.getRight())\n",
    "                queue.append(temp.getLeft())\n",
    "                counter += 2\n",
    "\n",
    "            else:\n",
    "                status = False\n",
    "        return root\n",
    " \n",
    "  def QvalTable(root, Pwt, Dt, Rtd, Rtc, k, gamma, alpha):\n",
    "        stack = [root]\n",
    "        while len(stack) != 0:\n",
    "            temp = stack.pop()\n",
    "\n",
    "            if temp.getDepth() == 3:\n",
    "                RF = temp.getTReward(Pwt, Dt, Rtd, Rtc, k, gamma)\n",
    "                temp.Qval += alpha * (RF - temp.Qval)\n",
    "\n",
    "                if len(temp.Gdata) == 0:\n",
    "                    temp.Gdata.append(temp.Qval)\n",
    "                elif int(temp.Qval) == int(temp.Gdata[len(temp.Gdata) - 1]):\n",
    "                    print(\"Optimal Qval for sequence:\" + str(temp) + \"is\")\n",
    "                    print(temp.Gdata)\n",
    "                    plt.show(temp.Gdata)\n",
    "                    plt.show()\n",
    "\n",
    "                #a=\"Optimal Qval for sequence:\"+str(temp)+\"is\"\n",
    "                #b=temp.Gdata\n",
    "                #for\n",
    "                #dic.keys()\n",
    "                #if len(dic)==8:\n",
    "                #    for i in dic.keys()\n",
    "                #        print i,dic[i]\n",
    "                else:\n",
    "                    temp.Gdata.append(temp.Qval)\n",
    "            if temp.getRight():\n",
    "                stack.append(temp.getRight())\n",
    "            if temp.getLeft():\n",
    "                stack.append(temp.getLeft())\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = [6000.0, 4800.0, 4800.0]\n",
    "D = [2800.0, 2800.0, 2800.0]\n",
    "k = 1\n",
    "g = 0.8\n",
    "Rd = 1000\n",
    "Rc = 1000\n",
    "al = 0.8\n",
    "d = input(\"Please Enter Depth of Prediction:\")\n",
    "mnode = DepnBtree(d)\n",
    "d = input(\"Enter the number of iterations:\")\n",
    "for i in range(int(d)):\n",
    "    QvalTable(mnode, P, D, Rd, Rc, k, g, al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q learning\n",
    "##### PyBrain : reinforcement learning,a tutorial \n",
    "using Q value Table\n",
    "\n",
    "source from [simon's technical blog](http://simontechblog.blogspot.com/2010/08/pybrain-reinforcement-learning-tutorial_21.html?showComment=1460952050532#c8338267566814532718)\n",
    "\n",
    "path : F:\\Anaconda2\\Lib\\site-packages\\pybrain\\rl\\environments\n",
    "\n",
    "dependencies：\n",
    "- *blackjackenv.py*\n",
    "- *blackjacktask.py*\n",
    ">即以下两个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybrain.rl.environments.environment import Environment\n",
    "from scipy import zeros\n",
    "\n",
    "class BlackjackEnv(Environment):\n",
    "    \"\"\" A (terribly simplified) Blackjack game implementation of an environment. \"\"\"       \n",
    "\n",
    "    # the number of action values the environment accepts\n",
    "    indim = 2\n",
    "    \n",
    "    # the number of sensor values the environment produces\n",
    "    outdim = 21\n",
    "    \n",
    "    def getSensors(self):\n",
    "        \"\"\" the currently visible state of the world (the    observation may be stochastic - repeated calls returning different values) \n",
    "            :rtype: by default, this is assumed to be a numpy array of doubles\n",
    "        \"\"\"\n",
    "        hand_value = int(raw_input(\"Enter hand value: \")) - 1\n",
    "        return [float(hand_value),]\n",
    "                    \n",
    "    def performAction(self, action):\n",
    "        \"\"\" perform an action on the world that changes it's internal state (maybe stochastically).\n",
    "            :key action: an action that should be executed in the Environment. \n",
    "            :type action: by default, this is assumed to be a numpy array of doubles\n",
    "        \"\"\"\n",
    "        print \"Action performed: \", action\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Most environments will implement this optional method that allows for reinitialization. \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import clip, asarray\n",
    "\n",
    "from pybrain.rl.environments.task import Task\n",
    "from numpy import *\n",
    "\n",
    "class BlackjackTask(Task):\n",
    "    \"\"\" A task is associating a purpose with an environment. It decides how to evaluate the observations, potentially returning \n",
    "        reinforcement rewards or fitness values. \n",
    "    Furthermore it is a filter for what should be visible to the agent.\n",
    "    Also, it can potentially act as a filter on how actions are transmitted to the environment. \"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        \"\"\" All tasks are coupled to an environment. \"\"\"\n",
    "        self.env = environment\n",
    "        # we will store the last reward given, remember that \"r\" in the Q learning formula is the one from the last interaction,\n",
    "        #    not the one given for the current interaction!\n",
    "        self.lastreward = 0\n",
    "\n",
    "    def performAction(self, action):\n",
    "        \"\"\" A filtered mapping towards performAction of the underlying environment. \"\"\"                \n",
    "        self.env.performAction(action)\n",
    "        \n",
    "    def getObservation(self):\n",
    "        \"\"\" A filtered mapping to getSample of the underlying environment. \"\"\"\n",
    "        sensors = self.env.getSensors()\n",
    "        return sensors\n",
    "    \n",
    "    def getReward(self):\n",
    "        \"\"\" Compute and return the current reward (i.e. corresponding to the last action performed) \"\"\"\n",
    "        reward = raw_input(\"Enter reward: \")\n",
    "        \n",
    "        # retrieve last reward, and save current given reward\n",
    "        cur_reward = self.lastreward\n",
    "        self.lastreward = reward\n",
    "    \n",
    "        return cur_reward\n",
    "\n",
    "    @property\n",
    "    def indim(self):\n",
    "        return self.env.indim\n",
    "    \n",
    "    @property\n",
    "    def outdim(self):\n",
    "        return self.env.outdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.rl.environments.blackjacktask import BlackjackTask\n",
    "from pybrain.rl.environments.blackjackenv import BlackjackEnv\n",
    "#above ,we should implemented it ourself \n",
    "\n",
    "from pybrain.rl.learners.valuebased import ActionValueTable\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "from pybrain.rl.learners import Q\n",
    "from pybrain.rl.experiments import Experiment\n",
    "from pybrain.rl.explorers import EpsilonGreedyExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define action-value table\n",
    "# number of states is:\n",
    "#\n",
    "#    current value: 1-21\n",
    "#\n",
    "# number of actions:\n",
    "#\n",
    "#    Stand=0, Hit=1\n",
    "av_table = ActionValueTable(21, 2)\n",
    "av_table.initialize(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print av_table\n",
    "# av_table??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define Q-learning agent\n",
    "learner = Q(0.5, 0.0)\n",
    "learner._setExplorer(EpsilonGreedyExplorer(0.0))\n",
    "agent = LearningAgent(av_table, learner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the environment\n",
    "env = BlackjackEnv()\n",
    "\n",
    "# define the task\n",
    "task = BlackjackTask(env)\n",
    "\n",
    "# finally, define experiment\n",
    "experiment = Experiment(task, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ready to go, start the process\n",
    "#while True:\n",
    "for i in range(300):\n",
    "    experiment.doInteractions(1)\n",
    "    agent.learn()\n",
    "    agent.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
