{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'\n",
    "\n",
    "from pybrain.rl.environments.cartpole import CartPoleEnvironment, DiscreteBalanceTask, CartPoleRenderer\n",
    "from pybrain.rl.agents import LearningAgent\n",
    "from pybrain.rl.experiments import EpisodicExperiment\n",
    "from pybrain.rl.learners.valuebased import NFQ, ActionValueNetwork\n",
    "from pybrain.rl.explorers import BoltzmannExplorer\n",
    "\n",
    "from numpy import array, arange, meshgrid, pi, zeros, mean\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# switch this to True if you want to see the cart balancing the pole (slower)\n",
    "render =True # False\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "env = CartPoleEnvironment()\n",
    "if render:\n",
    "    renderer = CartPoleRenderer()\n",
    "    env.setRenderer(renderer)\n",
    "    renderer.start()\n",
    "\n",
    "module = ActionValueNetwork(4, 3)\n",
    "\n",
    "task = DiscreteBalanceTask(env, 100)\n",
    "learner = NFQ()\n",
    "learner.explorer.epsilon = 0.4\n",
    "\n",
    "agent = LearningAgent(module, learner)\n",
    "testagent = LearningAgent(module, None)\n",
    "experiment = EpisodicExperiment(task, agent)\n",
    "\n",
    "\n",
    "def plotPerformance(values, fig):\n",
    "    plt.figure(fig.number)\n",
    "    plt.clf()\n",
    "    plt.plot(values, 'o-')\n",
    "    plt.gcf().canvas.draw()\n",
    "\n",
    "\n",
    "performance = []\n",
    "\n",
    "if not render:\n",
    "    pf_fig = plt.figure()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 100\n",
    "for i in xrange(1):\n",
    "    # one learning step after one episode of world-interaction\n",
    "    experiment.doEpisodes(1)\n",
    "    agent.learn(1)\n",
    "\n",
    "    # test performance (these real-world experiences are not used for training)\n",
    "    if render:\n",
    "        env.delay = True\n",
    "    experiment.agent = testagent\n",
    "    r = mean([sum(x) for x in experiment.doEpisodes(5)])\n",
    "    env.delay = False\n",
    "    testagent.reset()\n",
    "    experiment.agent = agent\n",
    "\n",
    "    performance.append(r)\n",
    "    if not render:\n",
    "        plotPerformance(performance, pf_fig)\n",
    "\n",
    "    print \"reward avg\", r\n",
    "    print \"explorer epsilon\", learner.explorer.epsilon\n",
    "    print \"num episodes\", agent.history.getNumSequences()\n",
    "    print \"update step\", len(performance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
